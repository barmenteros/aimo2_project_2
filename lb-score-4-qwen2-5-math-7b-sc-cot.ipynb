{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "I used a simple **Zero-Shot Self-Consistency Chain-of-Thought** approach using **Qwen2.5-math-7B-Instruct** model. I used this amazing [notebook](http://https://www.kaggle.com/code/takaito/aimo2-vllm-deepseek-math-7b-instruct-inference) by @[takaito](https://www.kaggle.com/takaito) as reference. \n",
    "\n",
    "Takaito's Notebook used a Greedy approach using deepseek-math model and scored 2. In this notebook we're using : \n",
    "* **Qwen2.5-math-7B-instruct model**\n",
    "* **256** Samples per Question\n",
    "* **L4x4** GPU\n",
    "* temperature **0.7**, max_tokens : **2048**\n",
    "\n",
    "Feel free to play with the params. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What else can be done : \n",
    "\n",
    "1. Add **TIR prompt** along with CoT\n",
    "2. USe multiple models, e.g. **Qwen2.5, Numina, Deepseek-math**. L4x4 have around **90+GB** vRAM. Sufficient to infer using multiple models\n",
    "3. Use TIR in a feedback manner (till now the best way to solve math problems, but surprisingly it's not doing great in this competition\n",
    "4. And much more! Sky is the limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caution**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This competition is much harder than the previous one. Obviously the questions are **AI Hard** in this competition, but another very important thing is, the Notebook run time limit is **5 hours** in this competition, whereas it was **9 hours** in the previous competition. This might be because we have **L4x4** GPUs now as an option along with T4x2/P100. That means we're quite dependent on L4 in this competition, becuase 5 hours with T4x2 is very low.\n",
    "\n",
    "For each problem we have around **5.5-5.8 minutes (330-350 seconds)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# notebook start time for timing purposes\n",
    "import time\n",
    "\n",
    "NB_START = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import libraries and modules\n",
    "import os\n",
    "import polars as pl\n",
    "import kaggle_evaluation.aimo_2_inference_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Since we're using 4 GPUs we need to set the CUDA_VISIBLE_DEVICES environment variable to \"0,1,2,3\" to make sure that the GPUs are visible to the code running in the notebook\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "%pip uninstall -y torch  # Remove existing PyTorch installation, -y flag auto-confirms removal\n",
    "\n",
    "%pip install -U --no-index --find-links=/kaggle/input/vllm-whl -U vllm  \n",
    "# Install vLLM from local wheels directory\n",
    "# -U: Upgrade if already installed\n",
    "# --no-index: Don't search PyPI\n",
    "# --find-links: Look for wheels in specified directory\n",
    "\n",
    "%pip install -U --upgrade /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "# Install specific version of grpcio package from local wheel file\n",
    "# Required for vLLM's RPC communication\n",
    "# Compatible with Python 3.10 and Linux x86_64\n",
    "\n",
    "%pip install -U --upgrade /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\n",
    "# Install specific version of Ray framework from local wheel file\n",
    "# Used by vLLM for distributed computing\n",
    "# Compatible with Python 3.10 and Linux x86_64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries and modules\n",
    "import gc  # Garbage collection module for managing memory allocation and deallocation\n",
    "import warnings  # Warning control module to suppress warnings in the notebook output\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress warnings\n",
    "import random  # Random number generation module for setting random seeds for reproducibility purposes\n",
    "import scipy as sp  # Scientific computing module for mathematical functions and operations on arrays and matrices\n",
    "import numpy as np  # Numerical computing module for working with arrays and matrices\n",
    "import pandas as pd  # Data manipulation and analysis module for working with data structures\n",
    "import math  # Mathematical functions module for mathematical operations\n",
    "from glob import glob  # File path pattern matching module for finding files in directories\n",
    "from pathlib import Path  # File path manipulation module for working with file paths\n",
    "import joblib  # Joblib module for parallel processing and caching\n",
    "import pickle  # Pickle module for serializing and deserializing Python objects\n",
    "import itertools  # Iteration module for efficient looping\n",
    "from tqdm import tqdm  # Progress bar module for tracking the progress of loops and tasks\n",
    "import re  # Regular expression module for pattern matching and string manipulation\n",
    "import vllm  # vLLM module for loading and using the vLLM language model for inference tasks and text generation tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the vLLM language model with the specified configuration settings and model path\n",
    "llm = vllm.LLM(\n",
    "    \"/kaggle/input/qwen7bmath\",  # \"deepseek-ai/deepseek-math-7b-instruct\" \n",
    "    # The number of GPUs to use for distributed execution with tensor parallelism (4 GPUs)\n",
    "    tensor_parallel_size=4,\n",
    "    # The ratio (between 0 and 1) of GPU memory to reserve for the model (0.95 - 95%)\n",
    "    gpu_memory_utilization=0.95,\n",
    "    # Trust remote code (e.g., from HuggingFace) when downloading the model and tokenizer (True)\n",
    "    trust_remote_code=True,\n",
    "    # Data type for model weights and activations (half precision) -> L4 supports bfloat16\n",
    "    # dtype=\"half\",\n",
    "    # Enable eager execution mode for debugging purposes (True)\n",
    "    enforce_eager=True,\n",
    "    # Swap space size in GB for storing model weights and activations (2 GB)\n",
    "    swap_space=2,\n",
    ")\n",
    "\n",
    "# Load the vLLM tokenizer for tokenizing input text data and converting it into input tokens for the language model\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to generate text using the vLLM language model with the specified requests, tokenizer, and model parameters\n",
    "# Temperature sampling used. You can try experimenting with different values of temperature to control the randomness in text generation.\n",
    "# Lower values of temperature (e.g., 0.1) will generate more deterministic and conservative text, while higher values of temperature (e.g., 1.0) will generate more diverse and creative text.\n",
    "# The max_tokens parameter controls the maximum number of tokens to generate in the output text. You can adjust this parameter based on the desired length of the generated text.\n",
    "def generate_text_vllm(requests, tokenizer, model):\n",
    "\n",
    "    # Sampling parameters for controlling text generation behavior and output length\n",
    "    sampling_params = vllm.SamplingParams(\n",
    "        # Sampling temperature for controlling randomness in text generation (0.7)\n",
    "        temperature=0.7,\n",
    "        # Maximum number of tokens to generate in the output text (2048)\n",
    "        max_tokens=2048,\n",
    "    )\n",
    "    # Generate text using the vLLM language model with the specified requests and sampling parameters\n",
    "    responses = model.generate(requests,\n",
    "                               sampling_params=sampling_params,\n",
    "                               use_tqdm=False)\n",
    "    # Initialize an empty list to store the generated text responses\n",
    "    response_text_list = ([])\n",
    "    # Iterate over the generated responses\n",
    "    for response in responses:\n",
    "        # Append the generated text to the response text list\n",
    "        response_text_list.append(response.outputs[0].text)\n",
    "    # Return the list of generated text responses\n",
    "    return response_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to extract the numerical answer from the text output generated by the vLLM language model.\n",
    "def naive_parse(answer):\n",
    "    out = []\n",
    "    start = False\n",
    "    end = False\n",
    "    # Reverse the text and iterate over the characters\n",
    "    for l in reversed(list(answer)):\n",
    "        # Check if the character is a digit and not at the end of the answer text\n",
    "        if (l in \"0123456789\" and not end):\n",
    "            start = True\n",
    "            out.append(l)\n",
    "        else:\n",
    "            if start:\n",
    "                end = True\n",
    "\n",
    "    out = reversed(out)\n",
    "    return \"\".join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Tool instruction for the CoT prompt (LaTeX format)\n",
    "tool_instruction = (\n",
    "    \"\\nPlease solve the problem above, and put your final answer within \\\\boxed{}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Counter module for counting occurrences of elements in a list or dictionary\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Function to determine the most consistent answer among the list of answers provided by the vLLM models for a given prompt. Our final answer will be the most consistent one among the possible ones.\n",
    "# The function uses the Counter module from the collections library to count the occurrences of each answer in the list and return the most common answer.\n",
    "# If there is a tie, the function returns the first answer in the list.\n",
    "# If the list is empty, the function returns 0.\n",
    "# The function also prints the most common answers and their counts for reference.\n",
    "def get_majority_vote(answers):\n",
    "\n",
    "    if not len(answers):\n",
    "        # Return 0 if the list is empty\n",
    "        return 0\n",
    "    # Count the occurrences of each answer in the list\n",
    "    c = Counter(answers)\n",
    "    # Get the most common answer and its count\n",
    "    value, _ = c.most_common()[0]\n",
    "    # Print the most common answers\n",
    "    print(\"Most Common answers : \", c.most_common()[:10])\n",
    "    # Print a separator line\n",
    "    print(\"=\" * 50)\n",
    "    # Try to convert the most common answer to an integer\n",
    "    try:\n",
    "        z = abs(value)\n",
    "    # If the conversion fails, set the answer to 0\n",
    "    except:\n",
    "        z = value\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Regular expression module for pattern matching and string manipulation\n",
    "import re\n",
    "\n",
    "\n",
    "# Function to extract the \\boxed{} answer from the generated text output using regular expressions and return the answer as an integer. If the answer extraction fails, the function returns -1.\n",
    "def find_answer(generate_text):\n",
    "\n",
    "    answer = -1\n",
    "\n",
    "    try:\n",
    "        # Extract the \\boxed{} answer using regular expressions\n",
    "        result_output = re.findall(r\"\\\\boxed\\{(\\d+)\\}\", generate_text)\n",
    "\n",
    "        # Check if the answer is found\n",
    "        if len(result_output) > 0:\n",
    "            # Parse the answer using the naive_parse function\n",
    "            no = naive_parse(result_output[0])\n",
    "            # Check if the parsed answer is not empty\n",
    "            if len(no) > 0:\n",
    "                # Convert the answer to an integer and take the modulo 1000\n",
    "                answer = int(no) % 1000\n",
    "\n",
    "            #print(answer)\n",
    "\n",
    "        else:\n",
    "            # Do nothing\n",
    "            ok = 1\n",
    "\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        #print(\"=\" * 100)\n",
    "        answer = -1\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Extract the answers from the generated texts. If the answer extraction fails, the function returns -1. The extracted answers are stored in a list.\n",
    "def extract_answer(texts):\n",
    "    sols = []\n",
    "    for text in texts:\n",
    "        try:\n",
    "            ans = find_answer(text)\n",
    "            if ans >= 0:\n",
    "                sols.append(ans)\n",
    "        except:\n",
    "            ans = -1\n",
    "    return sols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the final prediction from the list of solutions using majority voting.\n",
    "# If the list is empty, the function returns 0. Otherwise, it returns the majority vote.\n",
    "def fin_pred(sols):\n",
    "    if len(sols):\n",
    "        return get_majority_vote(sols)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Progress bar module for tracking the progress of loops and tasks\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_generations = 2  # per generation 128 samples -> around 2 minutes\n",
    "\n",
    "\n",
    "# Function to solve the given question using the vLLM language model and return the final answer.\n",
    "# The function generates text for the question prompt with the tool instruction and extracts the answers from the generated text.\n",
    "# The answers are then aggregated using majority voting to determine the final answer.\n",
    "# The function returns the final answer as an integer.\n",
    "def solve(question):\n",
    "    ans = []\n",
    "    for i in range(num_generations):\n",
    "        prompt = question + tool_instruction\n",
    "        generate_text = generate_text_vllm([prompt] * 128, tokenizer, llm)\n",
    "        ans.extend(extract_answer(generate_text))\n",
    "    answer = fin_pred(ans)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to make a prediction for the given question using the solve function. The function takes the question ID and the question text as input and returns the prediction as a DataFrame.\n",
    "def predict(id_: str, question: pl.Series) -> pl.DataFrame | pd.DataFrame:\n",
    "\n",
    "    # Extract the question text from the input Series\n",
    "    question = question.to_pandas().values[0]\n",
    "\n",
    "    # 4 hours 55 minutes limit for the notebook runtime (17700 seconds)\n",
    "    if time.time() - NB_START <= 17700:\n",
    "        try:\n",
    "            # Solve the question using the solve function\n",
    "            ans = solve(question)\n",
    "        except:\n",
    "            ans = 0\n",
    "    else:\n",
    "        ans = 0\n",
    "\n",
    "    # Return the prediction as a DataFrame\n",
    "    return pl.DataFrame({\"id\": id_, \"answer\": ans})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize the AIMO2InferenceServer with the predict function\n",
    "inference_server = kaggle_evaluation.aimo_2_inference_server.AIMO2InferenceServer(\n",
    "    predict)\n",
    "\n",
    "# Check if the code is running in the Kaggle competition environment\n",
    "if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "    # Start the inference server\n",
    "    inference_server.serve()\n",
    "\n",
    "# If the code is running in a local\n",
    "else:\n",
    "    # Run the local gateway with the test.csv file as input\n",
    "    inference_server.run_local_gateway(\n",
    "        (\"/kaggle/input/ai-mathematical-olympiad-progress-prize-2/test.csv\", ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you want to get the score on the reference set, set the value of **validate** to **True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set the validation flag to True to validate the model's predictions\n",
    "validate = False\n",
    "\n",
    "# Check if the code is running in the Kaggle competition environment\n",
    "if not os.getenv('KAGGLE_IS_COMPETITION_RERUN') and validate:\n",
    "    df = pd.read_csv(\n",
    "        \"/kaggle/input/ai-mathematical-olympiad-progress-prize-2/reference.csv\"\n",
    "    )\n",
    "    ans = []\n",
    "    cnt = 1\n",
    "    # For each problem in the dataset\n",
    "    for i in tqdm(df.problem.tolist()):\n",
    "        # Start time for each problem\n",
    "        tmp_time = time.time()\n",
    "        try:\n",
    "            # Solve the problem using the model and append the answer\n",
    "            ans.append(solve(i))\n",
    "            # Print the problem number and time taken\n",
    "            print(f\"Problem {cnt} solved. Time taken {time.time()-tmp_time}\")\n",
    "            cnt += 1\n",
    "        except:\n",
    "            # If the problem cannot be solved, append 0\n",
    "            ans.append(0)\n",
    "    # Add the model's answers to the dataframe\n",
    "    df[\"model_answer\"] = ans\n",
    "    # Check if the model's answers match the reference answers\n",
    "    df['match'] = df.answer == df.model_answer\n",
    "    # Print the number of matches and total examples\n",
    "    print(f'{df.match.sum()} matches in {len(df)} examples')\n",
    "    # Display the dataframe with the model's answers and matches\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 9869096,
     "sourceId": 86023,
     "sourceType": "competition"
    },
    {
     "datasetId": 4871830,
     "sourceId": 8218776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4746046,
     "sourceId": 8300737,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5898100,
     "sourceId": 9655281,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 49476,
     "modelInstanceId": 35171,
     "sourceId": 41850,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 107990,
     "modelInstanceId": 83725,
     "sourceId": 99805,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 141044,
     "modelInstanceId": 117826,
     "sourceId": 139150,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
