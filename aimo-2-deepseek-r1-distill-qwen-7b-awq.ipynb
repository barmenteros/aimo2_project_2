{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86023,"databundleVersionId":9869096,"sourceType":"competition"},{"sourceId":220483900,"sourceType":"kernelVersion"},{"sourceId":242129,"sourceType":"modelInstanceVersion","modelInstanceId":206829,"modelId":224053}],"dockerImageVersionId":30840,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":1915.293623,"end_time":"2024-12-14T00:30:49.092769","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-12-13T23:58:53.799146","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"255af715ce8746cdada24bd57410a109":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"394ecaf7716f4d258e338f875ae4c835":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f09de95b95f64817b8bdf28c7278a20d","placeholder":"​","style":"IPY_MODEL_bbd7a2e18f6d4a3a9642a3827b9ced38","value":""}},"41c7e147e597489b8791785b0472f4f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a26c2110bbc4546b8d2d74aa636bdf9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a3cde1f4e394b269d342d937829dd4f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a26c2110bbc4546b8d2d74aa636bdf9","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_255af715ce8746cdada24bd57410a109","value":5}},"8ba53add104149c4ba88b4138fd4498e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e268c538e6a48979b263677cbdaf279":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ba53add104149c4ba88b4138fd4498e","placeholder":"​","style":"IPY_MODEL_41c7e147e597489b8791785b0472f4f0","value":"Loading safetensors checkpoint shards: 100% Completed | 5/5 [02:13&lt;00:00, 28.89s/it]\n"}},"9e605e9455c24c8cac0ea9521feb3016":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_394ecaf7716f4d258e338f875ae4c835","IPY_MODEL_6a3cde1f4e394b269d342d937829dd4f","IPY_MODEL_8e268c538e6a48979b263677cbdaf279"],"layout":"IPY_MODEL_c1e5f9f23809451d918b10f2a03658c2"}},"bbd7a2e18f6d4a3a9642a3827b9ced38":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1e5f9f23809451d918b10f2a03658c2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f09de95b95f64817b8bdf28c7278a20d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\"\nimport re\nimport time\nimport random\nimport warnings\nfrom collections import Counter\nimport numpy as np, pandas as pd, polars as pl\n\nimport torch\nimport vllm\nfrom vllm import LLM, SamplingParams\n\nimport kaggle_evaluation.aimo_2_inference_server\n\nwarnings.simplefilter('ignore')\nprint('PyTorch version:', torch.__version__)\nprint('vLLM:', vllm.__version__)","metadata":{"papermill":{"duration":24.379545,"end_time":"2024-12-13T23:59:21.97668","exception":false,"start_time":"2024-12-13T23:58:57.597135","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.benchmark = True\n    torch.backends.cudnn.deterministic = True\nseed_everything(seed=0)\n\nstart_time = time.time()\ncutoff_time = start_time + (4 * 60 + 45) * 60\ncutoff_times = [int(x) for x in np.linspace(cutoff_time, start_time + 60 * 60, 50 + 1)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if os.getenv('KAGGLE_KERNEL_RUN_TYPE') or os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    llm_model_pth = '/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1'\nelse:\n    llm_model_pth = '/root/volume/KirillR/QwQ-32B-Preview-AWQ'\n\nMAX_NUM_SEQS = 32\nMAX_MODEL_LEN = 8192 * 3 // 2\n\nllm = LLM(\n    llm_model_pth,\n#    dtype=\"half\",                 # The data type for the model weights and activations\n    max_num_seqs=MAX_NUM_SEQS,    # Maximum number of sequences per iteration. Default is 256\n    max_model_len=MAX_MODEL_LEN,  # Model context length\n    trust_remote_code=True,       # Trust remote code (e.g., from HuggingFace) when downloading the model and tokenizer\n    tensor_parallel_size=4,       # The number of GPUs to use for distributed execution with tensor parallelism\n    gpu_memory_utilization=0.95,  # The ratio (between 0 and 1) of GPU memory to reserve for the model\n    seed=2024,\n)\n\ntokenizer = llm.get_tokenizer()","metadata":{"_kg_hide-output":true,"papermill":{"duration":237.271485,"end_time":"2024-12-14T00:03:19.252851","exception":false,"start_time":"2024-12-13T23:59:21.981366","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_boxed_text(text):\n    pattern = r'oxed{(.*?)}'\n    matches = re.findall(pattern, text)\n    if not matches:\n        return \"\"\n    for match in matches[::-1]:\n        if match != \"\":\n            return match\n    return \"\"\n\ndef batch_message_filter(list_of_messages) -> tuple[list[list[dict]], list[str]]:\n    extracted_answers = []\n    list_of_messages_to_keep = []\n    for messages in list_of_messages:\n        answer = extract_boxed_text(messages[-1]['content'])\n        if answer:\n            extracted_answers.append(answer)\n        else:\n            list_of_messages_to_keep.append(messages)\n    return list_of_messages_to_keep, extracted_answers\n\ndef select_answer(answers):\n    counter = Counter()\n    for answer in answers:\n        try:\n            if int(answer) == float(answer):\n                counter[int(answer)] += 1 + random.random() / 1_000\n        except:\n            pass\n    if not counter:\n        return 210\n    _, answer = sorted([(v,k) for k,v in counter.items()], reverse=True)[0]\n    return answer%1000\n\ndef batch_message_generate(list_of_messages) -> list[list[dict]]:\n    max_tokens = MAX_MODEL_LEN\n    if time.time() > cutoff_times[-1]:\n        print(\"Speedrun\")\n        max_tokens = 2 * MAX_MODEL_LEN // 3\n\n    sampling_params = SamplingParams(\n        temperature=1.0,               # Randomness of the sampling\n        top_p=0.90,                    # Cumulative probability of the top tokens to consider\n        min_p=0.05,                    # Minimum probability for a token to be considered\n        skip_special_tokens=True,      # Whether to skip special tokens in the output\n        max_tokens=max_tokens,         # Maximum number of tokens to generate\n        stop=[\"</think>\"],             # List of strings that stop the generation\n        seed=777,\n    )\n    \n    list_of_texts = [\n        tokenizer.apply_chat_template(\n            conversation=messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        for messages in list_of_messages\n    ]\n\n    request_output = llm.generate(\n        prompts=list_of_texts,\n        sampling_params=sampling_params,\n    )\n    print([len(single_request_output.outputs[0].token_ids) for single_request_output in request_output])\n\n    sort_keys_and_list_of_messages = []\n    for messages, single_request_output in zip(list_of_messages, request_output):\n        #print()\n        #print(single_request_output.outputs[0].text)\n        #print()\n        messages.append({'role': 'assistant', 'content': single_request_output.outputs[0].text})\n\n        sort_keys_and_list_of_messages.append(\n            (\n                len(single_request_output.outputs[0].token_ids),\n                messages\n            )\n        )\n    print([sort_key for sort_key, _ in sort_keys_and_list_of_messages])\n    sort_keys_and_list_of_messages.sort(key=lambda sort_key_and_messages: sort_key_and_messages[0])\n    print([sort_key for sort_key, _ in sort_keys_and_list_of_messages])\n    \n    list_of_messages = [messages for _, messages in sort_keys_and_list_of_messages]\n    return list_of_messages","metadata":{"papermill":{"duration":1.614384,"end_time":"2024-12-14T00:03:20.92038","exception":false,"start_time":"2024-12-14T00:03:19.305996","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_starter_messages(question, index):\n    options = []\n    for _ in range(13):\n        options.append(\n            [\n                {\"role\": \"system\", \"content\": \"You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step. Return final answer within \\\\boxed{}, after taking modulo 1000.\"},\n                {\"role\": \"user\", \"content\": question},\n            ]\n        )\n    for _ in range(3):    \n        options.append(\n            [\n                {\"role\": \"system\", \"content\": \"You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step. After you get your final answer, take modulo 1000, and return the final answer within \\\\boxed{}.\"},\n                {\"role\": \"user\", \"content\": question},\n            ],\n        )\n    return options[index%len(options)]\n\ndef predict_for_question(question: str) -> int:\n    selected_questions_only = True\n    #selected_questions_only = False\n    if selected_questions_only and not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n        #if \"Triangle\" not in question:\n        #    return 210\n        if \"Triangle\" not in question and \"delightful\" not in question and \"George\" not in question:\n            return 210\n\n    if time.time() > cutoff_time:\n        return 210\n    \n    print(question)\n\n    num_seqs = MAX_NUM_SEQS\n    if time.time() > cutoff_times[-1]:\n        num_seqs = 2 * MAX_NUM_SEQS // 3\n    \n    list_of_messages = [create_starter_messages(question, index) for index in range(num_seqs)]\n\n    all_extracted_answers = []\n    for _ in range(1):\n        list_of_messages = batch_message_generate(list_of_messages)\n        \n        if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n            df = pd.DataFrame(\n                {\n                    \"question\": [question] * len(list_of_messages),\n                    \"message\": [messages[-1][\"content\"] for messages in list_of_messages],\n                }\n            )\n            df.to_csv(f\"{str(int(time.time() - start_time)).zfill(5)}.csv\", index=False)\n        \n        list_of_messages, extracted_answers = batch_message_filter(list_of_messages)\n        all_extracted_answers.extend(extracted_answers)\n    \n    print(all_extracted_answers)\n    answer = select_answer(all_extracted_answers)\n    print(answer)\n\n    print(\"\\n\\n\")\n    cutoff_times.pop()\n    return answer\n\ndef predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n    id_ = id_.item(0)\n    print(\"------\")\n    print(id_)\n    question = question.item(0)\n    answer = predict_for_question(question)\n    print(question)\n    print(\"------\\n\\n\")\n    return pl.DataFrame({'id': id_, 'answer': answer})","metadata":{"papermill":{"duration":0.016248,"end_time":"2024-12-14T00:03:20.989048","exception":false,"start_time":"2024-12-14T00:03:20.9728","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#predict_for_question(\"Triangle $ABC$ has side length $AB = 120$ and circumradius $R = 100$. Let $D$ be the foot of the perpendicular from $C$ to the line $AB$. What is the greatest possible length of segment $CD$?\")","metadata":{"papermill":{"duration":0.012504,"end_time":"2024-12-14T00:03:21.030438","exception":false,"start_time":"2024-12-14T00:03:21.017934","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.read_csv(\n    '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/reference.csv'\n).drop('answer', axis=1).to_csv('reference.csv', index=False)","metadata":{"papermill":{"duration":0.070038,"end_time":"2024-12-14T00:03:21.108027","exception":false,"start_time":"2024-12-14T00:03:21.037989","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inference_server = kaggle_evaluation.aimo_2_inference_server.AIMO2InferenceServer(predict)\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        (\n#            '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/test.csv',\n            'reference.csv',\n        )\n    )","metadata":{"papermill":{"duration":1644.24778,"end_time":"2024-12-14T00:30:45.363503","exception":false,"start_time":"2024-12-14T00:03:21.115723","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}