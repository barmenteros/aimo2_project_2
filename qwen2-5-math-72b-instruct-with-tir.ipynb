{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ffc3a53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T02:51:38.263408Z",
     "iopub.status.busy": "2024-10-25T02:51:38.262446Z",
     "iopub.status.idle": "2024-10-25T02:51:38.269929Z",
     "shell.execute_reply": "2024-10-25T02:51:38.26859Z",
     "shell.execute_reply.started": "2024-10-25T02:51:38.263362Z"
    },
    "papermill": {
     "duration": 0.006898,
     "end_time": "2024-11-16T00:03:23.898213",
     "exception": false,
     "start_time": "2024-11-16T00:03:23.891315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "References\n",
    "\n",
    "- https://www.kaggle.com/code/richolson/ai-math-olympiad-qwen2-5-72b for showing how to submit\n",
    "- https://www.kaggle.com/code/abdullahmeda/load-72b-awq-model-using-vllm-on-l4-x4\n",
    "- https://www.kaggle.com/code/huikang/qwen2-5-math-1-5b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab52f42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:03:23.912025Z",
     "iopub.status.busy": "2024-11-16T00:03:23.911338Z",
     "iopub.status.idle": "2024-11-16T00:03:41.934219Z",
     "shell.execute_reply": "2024-11-16T00:03:41.933429Z"
    },
    "papermill": {
     "duration": 18.031768,
     "end_time": "2024-11-16T00:03:41.936182",
     "exception": false,
     "start_time": "2024-11-16T00:03:23.904414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os  # operating system\n",
    "import time\n",
    "import warnings\n",
    "import re  # regular expressions\n",
    "\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import polars as pl  # polars\n",
    "\n",
    "import torch  # pytorch\n",
    "import kaggle_evaluation.aimo_2_inference_server\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)  # display all columns\n",
    "cutoff_time = time.time() + (4 * 60 + 30) * 60  # 4 hours 30 minutes from now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fca8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def log_model_interaction(messages,\n",
    "                          response,\n",
    "                          log_file=\"model_interactions.jsonl\"):\n",
    "    \"\"\"\n",
    "    Log model interactions to a JSONL file.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dictionaries containing the input prompt\n",
    "        response: Model's response\n",
    "        log_file: Path to the log file\n",
    "    \"\"\"\n",
    "    log_entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"input_messages\": messages,\n",
    "        \"model_response\": response\n",
    "    }\n",
    "\n",
    "    # Write to JSONL file\n",
    "    with open(log_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(log_entry, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3858e4f",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-11-16T00:03:41.950030Z",
     "iopub.status.busy": "2024-11-16T00:03:41.949621Z",
     "iopub.status.idle": "2024-11-16T00:04:27.190263Z",
     "shell.execute_reply": "2024-11-16T00:04:27.189470Z"
    },
    "papermill": {
     "duration": 45.249755,
     "end_time": "2024-11-16T00:04:27.192425",
     "exception": false,
     "start_time": "2024-11-16T00:03:41.942670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 00:04:22,411\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "# VLLM model and sampling parameters for inference and evaluation\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# ignore warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# use all GPUs available on the Kaggle notebook (4 GPUs) for tensor parallelism with PyTorch and VLLM model inference\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "# disable tokenizers parallelism to avoid deadlocks with PyTorch tensor parallelism\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f19b67a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:04:27.206650Z",
     "iopub.status.busy": "2024-11-16T00:04:27.206046Z",
     "iopub.status.idle": "2024-11-16T00:10:26.631894Z",
     "shell.execute_reply": "2024-11-16T00:10:26.631031Z"
    },
    "papermill": {
     "duration": 359.435269,
     "end_time": "2024-11-16T00:10:26.634241",
     "exception": false,
     "start_time": "2024-11-16T00:04:27.198972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-16 00:04:27 config.py:1668] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 11-16 00:04:54 awq_marlin.py:97] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 11-16 00:04:54 config.py:905] Defaulting to use mp for distributed inference\n",
      "INFO 11-16 00:04:54 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/kaggle/input/qwen_qwen2.5_math_72bawq2/transformers/72b/3', speculative_config=None, tokenizer='/kaggle/input/qwen_qwen2.5_math_72bawq2/transformers/72b/3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=2024, served_model_name=/kaggle/input/qwen_qwen2.5_math_72bawq2/transformers/72b/3, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 11-16 00:04:54 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-16 00:04:54 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m INFO 11-16 00:04:57 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-16 00:04:57 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-16 00:04:57 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-16 00:04:58 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-16 00:04:58 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m INFO 11-16 00:04:58 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-16 00:04:58 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-16 00:04:58 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m INFO 11-16 00:04:58 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-16 00:04:58 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-16 00:04:58 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "WARNING 11-16 00:04:59 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m WARNING 11-16 00:04:59 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-16 00:04:59 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-16 00:04:59 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 11-16 00:04:59 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x78910be75960>, local_subscribe_port=39853, remote_subscribe_port=None)\n",
      "INFO 11-16 00:04:59 model_runner.py:1056] Starting to load model /kaggle/input/qwen_qwen2.5_math_72bawq2/transformers/72b/3...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m INFO 11-16 00:04:59 model_runner.py:1056] Starting to load model /kaggle/input/qwen_qwen2.5_math_72bawq2/transformers/72b/3...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m INFO 11-16 00:04:59 model_runner.py:1056] Starting to load model /kaggle/input/qwen_qwen2.5_math_72bawq2/transformers/72b/3...\n",
      "INFO 11-16 00:04:59 model_runner.py:1056] Starting to load model /kaggle/input/qwen_qwen2.5_math_72bawq2/transformers/72b/3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005bee3e5e224797aae923cc069af943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m INFO 11-16 00:10:10 model_runner.py:1067] Loading model weights took 9.7836 GB\n",
      "INFO 11-16 00:10:10 model_runner.py:1067] Loading model weights took 9.7836 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m INFO 11-16 00:10:10 model_runner.py:1067] Loading model weights took 9.7836 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m INFO 11-16 00:10:10 model_runner.py:1067] Loading model weights took 9.7836 GB\n",
      "INFO 11-16 00:10:17 distributed_gpu_executor.py:57] # GPU blocks: 8638, # CPU blocks: 3276\n",
      "INFO 11-16 00:10:17 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 33.74x\n",
      "INFO 11-16 00:10:21 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-16 00:10:21 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m INFO 11-16 00:10:21 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-16 00:10:21 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-16 00:10:21 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m INFO 11-16 00:10:21 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-16 00:10:21 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-16 00:10:21 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=357)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=355)\u001b[0;0m INFO 11-16 00:10:26 model_runner.py:1523] Graph capturing finished in 5 secs.\n",
      "INFO 11-16 00:10:26 model_runner.py:1523] Graph capturing finished in 5 secs.\n",
      "INFO 11-16 00:10:26 model_runner.py:1523] Graph capturing finished in 5 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=356)\u001b[0;0m INFO 11-16 00:10:26 model_runner.py:1523] Graph capturing finished in 5 secs.\n"
     ]
    }
   ],
   "source": [
    "# Get number of available GPUs for tensor parallelism\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "llm_model_pth = '/kaggle/input/qwen_qwen2.5_math_72bawq2/transformers/72b/3'\n",
    "\n",
    "# Load the VLLM model for inference\n",
    "llm = LLM(\n",
    "    llm_model_pth,\n",
    "    # The data type for the model weights and activations. Options: \"float\", \"half\". Default is \"float\"\n",
    "    dtype=\"half\",\n",
    "    # Maximum number of sequences per iteration. Default is 256 for \"float\" and 128 for \"half\"\n",
    "    max_num_seqs=16,  # Increased from 8 to 16\n",
    "    # Model context length. Default is 4096\n",
    "    max_model_len=4096,\n",
    "    # Trust remote code (e.g., from HuggingFace) when downloading the model and tokenizer. Default is False\n",
    "    trust_remote_code=True,\n",
    "    # The number of GPUs to use for distributed execution with tensor parallelism. Default is 4\n",
    "    tensor_parallel_size=4,\n",
    "    # Use half of the available GPUs for swap space\n",
    "    swap_space=max(1, num_gpus // 2),\n",
    "    # The ratio (between 0 and 1) of GPU memory to reserve for the model. Default is 0.95\n",
    "    gpu_memory_utilization=0.97,  # Increased from 0.95 to 0.97\n",
    "    # Random seed for reproducibility. Default is 2024\n",
    "    seed=2024,\n",
    "    # Enforce eager execution for debugging. Default is False\n",
    "    enforce_eager=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b97d0f00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:10:26.656933Z",
     "iopub.status.busy": "2024-11-16T00:10:26.656599Z",
     "iopub.status.idle": "2024-11-16T00:10:26.660336Z",
     "shell.execute_reply": "2024-11-16T00:10:26.659676Z"
    },
    "papermill": {
     "duration": 0.01685,
     "end_time": "2024-11-16T00:10:26.661946",
     "exception": false,
     "start_time": "2024-11-16T00:10:26.645096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the tokenizer for the VLLM model\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9def298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_python_code(text):\n",
    "    \"\"\"\n",
    "    Extracts Python code blocks from markdown text and combines them.\n",
    "    \n",
    "    Finds all text between ```python and ``` markers (markdown code blocks),\n",
    "    and joins multiple code blocks with double newlines.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text containing markdown-style Python code blocks\n",
    "        \n",
    "    Returns:\n",
    "        str: Concatenated Python code blocks, separated by double newlines\n",
    "        \n",
    "    Example:\n",
    "        Input:  '''\n",
    "                ```python\n",
    "                x = 1\n",
    "                ```\n",
    "                ```python\n",
    "                y = 2\n",
    "                ```\n",
    "                '''\n",
    "        Output: \"x = 1\\n\\ny = 2\"\n",
    "    \"\"\"\n",
    "    # Regular expression pattern for Python code blocks\n",
    "    pattern = r'```python\\s*(.*?)\\s*```'\n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    # Return the Python code blocks as a string separated by two newlines\n",
    "    return \"\\n\\n\".join(matches)  # Join all matches with two newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf23c390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keyword  # Python keywords\n",
    "\n",
    "\n",
    "def process_python_code(query):\n",
    "    \"\"\"\n",
    "    Preprocesses Python code by adding debug capabilities for mathematical problem solving.\n",
    "    \n",
    "    This function takes Python code (typically generated by an AI solving math problems) \n",
    "    and enhances it by:\n",
    "    1. Adding common math-related imports (math, numpy, sympy)\n",
    "    2. Adding automatic print statements for every top-level variable assignment\n",
    "    \n",
    "    Args:\n",
    "        query (str): Original Python code\n",
    "        \n",
    "    Returns:\n",
    "        str: Processed code with added imports and debug print statements\n",
    "    \n",
    "    Example:\n",
    "        Input:  \"x = 42\"\n",
    "        Output: \"import math\\\\nimport numpy as np\\\\nimport sympy as sp\\\\n\n",
    "                 x = 42\\\\n\n",
    "                 try:\\\\n    print(f\\\\\"x={str(x)[:100]}\\\\\")\\\\nexcept:\\\\n    pass\\\\n\"\n",
    "    \"\"\"\n",
    "    # Add import statements\n",
    "    # Also print variables if they are not inside any indentation\n",
    "    query = \"import math\\nimport numpy as np\\nimport sympy as sp\\n\" + query\n",
    "    # Split the query into rows\n",
    "    current_rows = query.strip().split(\"\\n\")\n",
    "    new_rows = []\n",
    "    for row in current_rows:\n",
    "        # Add the current row\n",
    "        new_rows.append(row)\n",
    "        if not row.startswith(\" \") and \"=\" in row:\n",
    "            # Get the variable name\n",
    "            variables_to_print = row.split(\"=\")[0].strip()\n",
    "            # Split multiple variables\n",
    "            for variable_to_print in variables_to_print.split(\",\"):\n",
    "                # Remove leading/trailing spaces\n",
    "                variable_to_print = variable_to_print.strip()\n",
    "                # Check if the variable is a valid identifier and not a Python keyword\n",
    "                if variable_to_print.isidentifier(\n",
    "                ) and not keyword.iskeyword(variable_to_print):\n",
    "                    if row.count(\"(\") == row.count(\")\") and row.count(\n",
    "                            \"[\") == row.count(\"]\"):\n",
    "                        # TODO: use some AST to parse code\n",
    "                        new_rows.append(\n",
    "                            f'\\ntry:\\n    print(f\"{variable_to_print}={{str({variable_to_print})[:100]}}\")\\nexcept:\\n    pass\\n'\n",
    "                        )\n",
    "    return \"\\n\".join(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35894c7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:10:26.684072Z",
     "iopub.status.busy": "2024-11-16T00:10:26.683814Z",
     "iopub.status.idle": "2024-11-16T00:10:26.694095Z",
     "shell.execute_reply": "2024-11-16T00:10:26.693445Z"
    },
    "papermill": {
     "duration": 0.023215,
     "end_time": "2024-11-16T00:10:26.695640",
     "exception": false,
     "start_time": "2024-11-16T00:10:26.672425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract text inside \"boxed\" curly braces {text} using regular expressions (regex) and return it as a string or\n",
    "# an empty string if no match is found (e.g., \"{text}\" will return \"text\")\n",
    "def extract_boxed_text(text):\n",
    "    # Regular expression pattern for boxed text\n",
    "    pattern = r'oxed{(.*?)}'\n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    # Return the first match\n",
    "    return matches[0]\n",
    "\n",
    "\n",
    "from collections import Counter  # Counter for counting occurrences of elements in a list\n",
    "import random  # random numbers and shuffling lists (e.g., for random sampling)\n",
    "\n",
    "\n",
    "# Select the most common answer from a list of answers and return it as an integer (e.g., [1, 2, 2, 3, 3, 3] will return 3)\n",
    "def select_answer(answers):\n",
    "    # Counter for counting occurrences of elements\n",
    "    counter = Counter()\n",
    "    for answer in answers:\n",
    "        try:\n",
    "            # Check if the answer is an integer\n",
    "            if int(answer) == float(answer):\n",
    "                # Add the answer to the counter with a small random noise to break ties\n",
    "                counter[int(answer)] += 1 + random.random() / 1_000\n",
    "        except:\n",
    "            pass\n",
    "    if not counter:\n",
    "        # Return the default answer if no valid answers are found\n",
    "        return 210\n",
    "    # Select the most common answer from the counter\n",
    "    _, answer = sorted([(v, k) for k, v in counter.items()], reverse=True)[0]\n",
    "    # Return the answer modulo 1000 (e.g., 1000 will be returned as 0)\n",
    "    # TODO: print the answer before modulo 1000 to check if it is correct\n",
    "    return answer % 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6718b62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:10:26.717808Z",
     "iopub.status.busy": "2024-11-16T00:10:26.717188Z",
     "iopub.status.idle": "2024-11-16T00:10:26.725130Z",
     "shell.execute_reply": "2024-11-16T00:10:26.724492Z"
    },
    "papermill": {
     "duration": 0.020748,
     "end_time": "2024-11-16T00:10:26.726770",
     "exception": false,
     "start_time": "2024-11-16T00:10:26.706022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile  # temporary files and directories\n",
    "import subprocess  # subprocesses\n",
    "\n",
    "\n",
    "# Python Read-Eval-Print Loop (REPL) for executing Python code with a timeout using subprocesses and temporary \n",
    "# files and directories for security and resource management\n",
    "class PythonREPL:\n",
    "\n",
    "    def __init__(self, timeout=5):\n",
    "        # Timeout for code execution in seconds\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def __call__(self, query):\n",
    "        # Create a temporary directory\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Temporary Python file path\n",
    "            temp_file_path = os.path.join(temp_dir, \"tmp.py\")\n",
    "            # Write the query to the temporary file\n",
    "            with open(temp_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(query)\n",
    "\n",
    "            try:\n",
    "                # Execute the Python file with a timeout\n",
    "                result = subprocess.run(\n",
    "                    [\"python3\", temp_file_path],\n",
    "                    # Capture stdout and stderr\n",
    "                    capture_output=True,\n",
    "                    # Do not raise an exception on non-zero exit codes\n",
    "                    check=False,\n",
    "                    # Return stdout and stderr as text\n",
    "                    text=True,\n",
    "                    # Timeout for code execution\n",
    "                    timeout=self.timeout,\n",
    "                )\n",
    "            except subprocess.TimeoutExpired:\n",
    "                return False, f\"Execution timed out after {self.timeout} seconds.\"\n",
    "\n",
    "            stdout = result.stdout.strip()\n",
    "            stderr = result.stderr.strip()\n",
    "\n",
    "            if result.returncode == 0:\n",
    "                return True, stdout\n",
    "            else:\n",
    "                # Process the error message to remove the temporary file path\n",
    "                # This makes the error message cleaner and more user-friendly\n",
    "                error_lines = stderr.split(\"\\n\")\n",
    "                cleaned_errors = []\n",
    "                for line in error_lines:\n",
    "                    if temp_file_path in line:\n",
    "                        # Remove the path from the error line\n",
    "                        line = line.replace(temp_file_path, \"<temporary_file>\")\n",
    "                    cleaned_errors.append(line)\n",
    "                cleaned_error_msg = \"\\n\".join(cleaned_errors)\n",
    "                # Include stdout in the error case\n",
    "                combined_output = f\"{stdout}\\n{cleaned_error_msg}\" if stdout else cleaned_error_msg\n",
    "                return False, combined_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1d2380",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:10:26.748868Z",
     "iopub.status.busy": "2024-11-16T00:10:26.748299Z",
     "iopub.status.idle": "2024-11-16T00:10:26.754432Z",
     "shell.execute_reply": "2024-11-16T00:10:26.753794Z"
    },
    "papermill": {
     "duration": 0.018873,
     "end_time": "2024-11-16T00:10:26.756027",
     "exception": false,
     "start_time": "2024-11-16T00:10:26.737154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sampling parameters for the VLLM model inference and evaluation (e.g., temperature, min_p, max_tokens, etc.)\n",
    "sampling_params = SamplingParams(\n",
    "    # The temperature of the sampling distribution. Higher values mean more randomness. Default is 1.0\n",
    "    temperature=0.7,  # Changed from 1.0\n",
    "    # The minimum token probability for nucleus sampling. Default is 0.01\n",
    "    # min_p=0.01,\n",
    "    # The maximum token probability for nucleus sampling. Default is 1.0\n",
    "    top_p=0.8,\n",
    "    # Whether to skip special tokens in the output. Default is True\n",
    "    #skip_special_tokens=True,\n",
    "    # Maximum number of tokens to generate. Default is 1024\n",
    "    max_tokens=3072,  # Changed from 2400\n",
    "    # Stop generation at the end of the code block\n",
    "    stop=[\"```\\n\"],\n",
    "    # Include the stop string in the output. Default is False\n",
    "    include_stop_str_in_output=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Generate a message using the VLLM model and return the generated message as a string (e.g., \"Hello, world!\")\n",
    "# or an empty string if no message is generated (e.g., if the input is empty)\n",
    "def batch_message_generate(list_of_messages) -> list[list[dict]]:\n",
    "\n",
    "    list_of_texts = [\n",
    "        # Apply the chat template to each conversation and add the generation prompt to each message in the conversation\n",
    "        # (e.g., \"role: user\\ncontent: Hello, world!\") and return the list of texts as a list of strings\n",
    "        # (e.g., [\"role: user\\ncontent: Hello, world!\", \"role: assistant\\ncontent: Hi!\"])\n",
    "        tokenizer.apply_chat_template(conversation=messages,\n",
    "                                      tokenize=False,\n",
    "                                      add_generation_prompt=True)\n",
    "        for messages in list_of_messages\n",
    "    ]\n",
    "\n",
    "    # Log the input prompts\n",
    "    for messages, full_prompt in zip(list_of_messages, list_of_texts):\n",
    "        print(\"\\n=== Input Prompt ===\")\n",
    "        print(full_prompt)\n",
    "        print(\"==================\\n\")\n",
    "\n",
    "    # Generate messages using the VLLM model with the list of texts and sampling parameters\n",
    "    request_output = llm.generate(\n",
    "        prompts=list_of_texts,\n",
    "        sampling_params=sampling_params,\n",
    "    )\n",
    "\n",
    "    # Process and log the outputs\n",
    "    for messages, single_request_output in zip(list_of_messages,\n",
    "                                               request_output):\n",
    "        response_text = single_request_output.outputs[0].text\n",
    "\n",
    "        print(\"\\n=== Model Response ===\")\n",
    "        print(response_text)\n",
    "        print(\"=====================\\n\")\n",
    "\n",
    "        # Log to file\n",
    "        log_model_interaction(messages, response_text)\n",
    "\n",
    "        messages.append({'role': 'assistant', 'content': response_text})\n",
    "\n",
    "    return list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f0d7342",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:10:26.778196Z",
     "iopub.status.busy": "2024-11-16T00:10:26.777625Z",
     "iopub.status.idle": "2024-11-16T00:10:26.782030Z",
     "shell.execute_reply": "2024-11-16T00:10:26.781387Z"
    },
    "papermill": {
     "duration": 0.017004,
     "end_time": "2024-11-16T00:10:26.783582",
     "exception": false,
     "start_time": "2024-11-16T00:10:26.766578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter messages that contain boxed text and extract the boxed text as the answer\n",
    "def batch_message_filter(\n",
    "        list_of_messages) -> tuple[list[list[dict]], list[str]]:\n",
    "    extracted_answers = []\n",
    "    list_of_messages_to_keep = []\n",
    "    for messages in list_of_messages:\n",
    "        answer = extract_boxed_text(messages[-1]['content'])\n",
    "        if answer:\n",
    "            extracted_answers.append(answer)\n",
    "        else:\n",
    "            list_of_messages_to_keep.append(messages)\n",
    "    return list_of_messages_to_keep, extracted_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "027092c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:10:26.805645Z",
     "iopub.status.busy": "2024-11-16T00:10:26.805076Z",
     "iopub.status.idle": "2024-11-16T00:10:26.811371Z",
     "shell.execute_reply": "2024-11-16T00:10:26.810757Z"
    },
    "papermill": {
     "duration": 0.019087,
     "end_time": "2024-11-16T00:10:26.812953",
     "exception": false,
     "start_time": "2024-11-16T00:10:26.793866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Execute Python code in messages and return the output as a string (e.g., \"Hello, world!\")\n",
    "# or an empty string if no output is generated (e.g., if the input is empty)\n",
    "# or an error occurs (e.g., syntax error) during code execution (e.g., \"SyntaxError: invalid syntax\")\n",
    "# or if the code execution times out (e.g., \"Execution timed out after 5 seconds.\")\n",
    "# or if the code execution fails (e.g., \"Execution failed with exit code 1.\")\n",
    "# or if the code execution is successful but no output is generated (e.g., \"Execution successful but no output.\")\n",
    "# or if the code execution is successful but the output is empty (e.g., \"Execution successful but empty output.\")\n",
    "# or if the code execution is successful but the output is too long (e.g., \"Execution successful but output is too long.\")\n",
    "# or if the code execution is successful but the output is too short (e.g., \"Execution successful but output is too short.\")\n",
    "# or if the code execution is successful but the output is invalid (e.g., \"Execution successful but invalid output.\")\n",
    "# or if the code execution is successful but the output is not a string (e.g., \"Execution successful but output is not a string.\")\n",
    "# or if the code execution is successful but the output is not a valid answer\n",
    "def batch_message_execute(list_of_messages) -> list[list[dict]]:\n",
    "    for messages in list_of_messages:\n",
    "        python_code = extract_python_code(messages[-1]['content'])\n",
    "        python_code = process_python_code(python_code)\n",
    "        # print('\\n\\n' + python_code + '\\n\\n')\n",
    "        try:\n",
    "            print('c', end='')\n",
    "            is_successful, output = PythonREPL()(python_code)\n",
    "            if is_successful:\n",
    "                print('o', end='')\n",
    "            else:\n",
    "                print('e', end='')\n",
    "        except Exception as e:\n",
    "            print('f', end='')\n",
    "            output = str(e)\n",
    "        print(python_code)\n",
    "        print()\n",
    "        print(output)\n",
    "        print(\"\\n\\n\")\n",
    "        messages.append({\n",
    "            'role': 'user',\n",
    "            'content': \"```output\\n\" + output + \"\\n```\"\n",
    "        })\n",
    "    print()\n",
    "    return list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e265ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:10:26.834746Z",
     "iopub.status.busy": "2024-11-16T00:10:26.834197Z",
     "iopub.status.idle": "2024-11-16T00:10:26.838858Z",
     "shell.execute_reply": "2024-11-16T00:10:26.838211Z"
    },
    "papermill": {
     "duration": 0.017196,
     "end_time": "2024-11-16T00:10:26.840454",
     "exception": false,
     "start_time": "2024-11-16T00:10:26.823258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_starter_messages(question, index):\n",
    "    cycle_size = 2\n",
    "    if False:\n",
    "        pass\n",
    "    elif index % cycle_size == 1:\n",
    "        # https://github.com/QwenLM/Qwen2.5-Math?tab=readme-ov-file#-hugging-face-transformers\n",
    "        # CoT\n",
    "        return [{\n",
    "            \"role\":\n",
    "            \"system\",\n",
    "            \"content\":\n",
    "            \"Please reason step by step, and put your final answer within \\\\boxed{}.\"\n",
    "        }, {\n",
    "            \"role\":\n",
    "            \"user\",\n",
    "            \"content\":\n",
    "            question + \"\\n\\nBegin your answer by importing math and sympy.\"\n",
    "        }]\n",
    "    else:\n",
    "        # https://github.com/QwenLM/Qwen2.5-Math?tab=readme-ov-file#-hugging-face-transformers\n",
    "        # TIR\n",
    "        return [{\n",
    "            \"role\": \n",
    "            \"system\",\n",
    "            \"content\":\n",
    "            \"Please integrate natural language reasoning with programs to solve the problem above, and put your final answer within \\\\boxed{}.\"\n",
    "        }, {\n",
    "            \"role\":\n",
    "            \"user\",\n",
    "            \"content\":\n",
    "            question + \"\\n\\nBegin your answer by importing math and sympy.\"\n",
    "        }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21b967ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:10:26.862277Z",
     "iopub.status.busy": "2024-11-16T00:10:26.862014Z",
     "iopub.status.idle": "2024-11-16T00:10:26.868468Z",
     "shell.execute_reply": "2024-11-16T00:10:26.867841Z"
    },
    "papermill": {
     "duration": 0.019122,
     "end_time": "2024-11-16T00:10:26.870088",
     "exception": false,
     "start_time": "2024-11-16T00:10:26.850966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dynamic Batch Size Optimization\n",
    "-----------------------------\n",
    "The following implementation replaces the static batch size of 32 with a dynamic batch sizing mechanism.\n",
    "\n",
    "Rationale:\n",
    "- Fixed batch sizes can be suboptimal as GPU memory availability varies during execution\n",
    "- Large static batch sizes may cause OOM errors or memory fragmentation\n",
    "- Small static batch sizes may underutilize available resources\n",
    "- Memory requirements per conversation can vary based on problem complexity\n",
    "\n",
    "Benefits:\n",
    "- Adaptive resource utilization based on real-time GPU memory availability\n",
    "- Reduced risk of OOM errors during long running sessions\n",
    "- Better throughput by maximizing parallel processing when possible\n",
    "- More resilient to varying problem complexities\n",
    "\n",
    "Implementation notes:\n",
    "- Uses 80% of available GPU memory to leave headroom for fluctuations\n",
    "- Sets reasonable min/max bounds (8-48) to maintain stability\n",
    "- Falls back to conservative batch size (16) if memory detection fails\n",
    "- Considers multi-GPU setups by checking all available devices\n",
    "\n",
    "Last modified: November 2024\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_optimal_batch_size():\n",
    "    \"\"\"\n",
    "    Dynamically determine the optimal batch size for parallel message processing\n",
    "    based on available GPU memory across all devices.\n",
    "    \n",
    "    The function:\n",
    "    1. Checks available memory across all GPU devices\n",
    "    2. Calculates safe batch size using conservative memory estimates\n",
    "    3. Applies bounds to ensure stable execution\n",
    "    \n",
    "    Returns:\n",
    "        int: Optimal batch size between 8 and 48\n",
    "        \n",
    "    Note:\n",
    "        - Assumes ~0.5GB memory usage per conversation\n",
    "        - Uses 80% of available memory as safety margin\n",
    "        - Falls back to batch size 16 if memory detection fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get available GPU memory\n",
    "        gpu_memory = []\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            total_memory = torch.cuda.get_device_properties(i).total_memory\n",
    "            allocated_memory = torch.cuda.memory_allocated(i)\n",
    "            free_memory = total_memory - allocated_memory\n",
    "            gpu_memory.append(free_memory)\n",
    "\n",
    "        # Use the minimum free memory across all GPUs\n",
    "        min_free_memory = min(gpu_memory)\n",
    "\n",
    "        # Calculate batch size based on free memory\n",
    "        # Conservative estimate: assume each conversation requires about 0.5GB\n",
    "        memory_per_conversation = 0.5 * (1024**3)  # 0.5GB in bytes\n",
    "        optimal_batch_size = int(\n",
    "            (min_free_memory * 0.8) /\n",
    "            memory_per_conversation)  # Use 80% of free memory\n",
    "\n",
    "        # Ensure batch size is within reasonable bounds\n",
    "        optimal_batch_size = max(8, min(48, optimal_batch_size))\n",
    "\n",
    "        return optimal_batch_size\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating batch size: {e}\")\n",
    "        return 16  # Default fallback batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8da89aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:10:26.892177Z",
     "iopub.status.busy": "2024-11-16T00:10:26.891631Z",
     "iopub.status.idle": "2024-11-16T00:10:26.897976Z",
     "shell.execute_reply": "2024-11-16T00:10:26.897325Z"
    },
    "papermill": {
     "duration": 0.018983,
     "end_time": "2024-11-16T00:10:26.899551",
     "exception": false,
     "start_time": "2024-11-16T00:10:26.880568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_for_question(question: str) -> int:\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\nNEW QUESTION\\n\" + \"=\" * 80)\n",
    "\n",
    "    import os\n",
    "    # only run this code if it's not a competition rerun\n",
    "    if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        # only run this code if the question is not the example question from the competition page (to avoid wasting time)\n",
    "        if question != \"Triangle $ABC$ has side length $AB = 120$ and circumradius $R = 100$. Let $D$ be the foot of the perpendicular from $C$ to the line $AB$. What is the greatest possible length of segment $CD$?\":\n",
    "            return 210\n",
    "    # check if the time limit has been reached\n",
    "    if time.time() > cutoff_time:\n",
    "        return 210\n",
    "\n",
    "    question += \"\\nIf the final answer is a number larger than 1 million, take modulo 1000.\"\n",
    "    print(question)\n",
    "\n",
    "    # Dynamic batch size instead of fixed 32 for parallel message processing (e.g., for tensor parallelism) based on available GPU memory across all devices (e.g., 8-48 conversations in parallel)\n",
    "    batch_size = get_optimal_batch_size()\n",
    "    list_of_messages = [\n",
    "        create_starter_messages(question, index) for index in range(batch_size)\n",
    "    ]\n",
    "\n",
    "    all_extracted_answers = []\n",
    "    # 4 rounds of message generation, filtering, and execution\n",
    "    for _ in range(4):\n",
    "        # Generate messages using the VLLM model\n",
    "        list_of_messages = batch_message_generate(list_of_messages)\n",
    "        # Filter messages that contain boxed text and extract the boxed text as the answer\n",
    "        list_of_messages, extracted_answers = batch_message_filter(\n",
    "            list_of_messages)\n",
    "        # Extend the list of extracted answers\n",
    "        all_extracted_answers.extend(extracted_answers)\n",
    "        if not list_of_messages:\n",
    "            break\n",
    "        # Execute Python code in messages and return the output as a string\n",
    "        list_of_messages = batch_message_execute(list_of_messages)\n",
    "\n",
    "    print(all_extracted_answers)\n",
    "    answer = select_answer(all_extracted_answers)\n",
    "    print(answer)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eec2bf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:10:26.921092Z",
     "iopub.status.busy": "2024-11-16T00:10:26.920841Z",
     "iopub.status.idle": "2024-11-16T00:10:26.925292Z",
     "shell.execute_reply": "2024-11-16T00:10:26.924666Z"
    },
    "papermill": {
     "duration": 0.017005,
     "end_time": "2024-11-16T00:10:26.926884",
     "exception": false,
     "start_time": "2024-11-16T00:10:26.909879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace this function with your inference code.\n",
    "# The function should return a single integer between 0 and 999, inclusive.\n",
    "# Each prediction (except the very first) must be returned within 30 minutes of the question being provided.\n",
    "def predict(id_: pl.DataFrame,\n",
    "            question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
    "    # get the first element of the DataFrame\n",
    "    id_ = id_.item(0)\n",
    "    print(\"------\")\n",
    "    print(id_)\n",
    "\n",
    "    # get the first element of the DataFrame\n",
    "    question = question.item(0)\n",
    "    answer = predict_for_question(question)\n",
    "    print(question)\n",
    "    print(\"------\\n\\n\\n\")\n",
    "    return pl.DataFrame({'id': id_, 'answer': answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c34de22c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:10:26.949528Z",
     "iopub.status.busy": "2024-11-16T00:10:26.948925Z",
     "iopub.status.idle": "2024-11-16T00:10:26.952307Z",
     "shell.execute_reply": "2024-11-16T00:10:26.951683Z"
    },
    "papermill": {
     "duration": 0.016078,
     "end_time": "2024-11-16T00:10:26.953890",
     "exception": false,
     "start_time": "2024-11-16T00:10:26.937812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict_for_question(\"Triangle $ABC$ has side length $AB = 120$ and circumradius $R = 100$. Let $D$ be the foot of the perpendicular from $C$ to the line $AB$. What is the greatest possible length of segment $CD$?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e34c3214",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:10:26.975902Z",
     "iopub.status.busy": "2024-11-16T00:10:26.975422Z",
     "iopub.status.idle": "2024-11-16T00:10:27.032673Z",
     "shell.execute_reply": "2024-11-16T00:10:27.031977Z"
    },
    "papermill": {
     "duration": 0.070097,
     "end_time": "2024-11-16T00:10:27.034433",
     "exception": false,
     "start_time": "2024-11-16T00:10:26.964336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\n",
    "    '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/reference.csv'\n",
    ").drop('answer', axis=1).to_csv('reference.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18f6ec46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:10:27.056592Z",
     "iopub.status.busy": "2024-11-16T00:10:27.056332Z",
     "iopub.status.idle": "2024-11-16T00:16:20.433439Z",
     "shell.execute_reply": "2024-11-16T00:16:20.432642Z"
    },
    "papermill": {
     "duration": 353.39044,
     "end_time": "2024-11-16T00:16:20.435392",
     "exception": false,
     "start_time": "2024-11-16T00:10:27.044952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "bbd91e\n",
      "Alice writes all positive integers from $1$ to $n$ on the board for some positive integer $n \\geq 11$. Bob then erases ten of them. The mean of the remaining numbers is $3000/37$. The sum of the numbers Bob erased is $S$. What is the remainder when $n \\times S$ is divided by $997$?\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "057f8a\n",
      "Three airline companies operate flights from Dodola island. Each company has a different schedule of departures. The first company departs every 100 days, the second every 120 days and the third every 150 days. What is the greatest positive integer $d$ for which it is true that there will be $d$ consecutive days without a flight from Dodola island, regardless of the departure times of the various airlines?\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "480182\n",
      "Let $ABC$ be a triangle with $BC=108$, $CA=126$, and $AB=39$. Point $X$ lies on segment $AC$ such that $BX$ bisects $\\angle CBA$. Let $\\omega$ be the circumcircle of triangle $ABX$. Let $Y$ be a point on $\\omega$ different from $X$ such that $CX=CY$. Line $XY$ meets $BC$ at $E$. The length of the segment $BE$ can be written as $\\frac{m}{n}$, where $m$ and $n$ are coprime positive integers. Find $m+n$.\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "349493\n",
      "We call a sequence $a_1, a_2, \\ldots$ of non-negative integers \\textit{delightful} if there exists a positive integer $N$ such that for all $n > N$, $a_n = 0$, and for all $i \\geq 1$, $a_i$ counts the number of multiples of $i$ in $a_1, a_2, \\ldots, a_N$. How many delightful sequences of non-negative integers are there?\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "71beb6\n",
      "For a positive integer $n$, let $S(n)$ denote the sum of the digits of $n$ in base 10. Compute $S(S(1)+S(2)+\\cdots+S(N))$ with $N=10^{100}-2$.\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "88c219\n",
      "For positive integers $x_1,\\ldots, x_n$ define $G(x_1, \\ldots, x_n)$ to be the sum of their $\\frac{n(n-1)}{2}$ pairwise greatest common divisors. We say that an integer $n \\geq 2$ is \\emph{artificial} if there exist $n$ different positive integers $a_1, ..., a_n$ such that \n",
      "\\[a_1 + \\cdots + a_n = G(a_1, \\ldots, a_n) +1.\\]\n",
      "Find the sum of all artificial integers $m$ in the range $2 \\leq m \\leq 40$.\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "1acac0\n",
      "Triangle $ABC$ has side length $AB = 120$ and circumradius $R = 100$. Let $D$ be the foot of the perpendicular from $C$ to the line $AB$. What is the greatest possible length of segment $CD$?\n",
      "If the final answer is a number larger than 1 million, take modulo 1000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 8/8 [02:33<00:00, 19.24s/it, est. speed input: 5.95 toks/s, output: 105.00 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coimport math\n",
      "import numpy as np\n",
      "import sympy as sp\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "coimport math\n",
      "import numpy as np\n",
      "import sympy as sp\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "coimport math\n",
      "import numpy as np\n",
      "import sympy as sp\n",
      "import sympy as sp\n",
      "\n",
      "# Define the sides a and b\n",
      "a = sp.Symbol('a', positive=True, real=True)\n",
      "\n",
      "try:\n",
      "    print(f\"a={str(a)[:100]}\")\n",
      "except:\n",
      "    pass\n",
      "\n",
      "b = sp.Symbol('b', positive=True, real=True)\n",
      "\n",
      "try:\n",
      "    print(f\"b={str(b)[:100]}\")\n",
      "except:\n",
      "    pass\n",
      "\n",
      "\n",
      "# Given side c and circumradius R\n",
      "c = 120\n",
      "\n",
      "try:\n",
      "    print(f\"c={str(c)[:100]}\")\n",
      "except:\n",
      "    pass\n",
      "\n",
      "R = 100\n",
      "\n",
      "try:\n",
      "    print(f\"R={str(R)[:100]}\")\n",
      "except:\n",
      "    pass\n",
      "\n",
      "\n",
      "# Expression for area K in terms of sides and circumradius\n",
      "K = (a * b * c) / (4 * R)\n",
      "\n",
      "try:\n",
      "    print(f\"K={str(K)[:100]}\")\n",
      "except:\n",
      "    pass\n",
      "\n",
      "\n",
      "# Expression for area K in terms of base and height\n",
      "CD = sp.Symbol('CD')\n",
      "\n",
      "try:\n",
      "    print(f\"CD={str(CD)[:100]}\")\n",
      "except:\n",
      "    pass\n",
      "\n",
      "K_CD = (1/2) * c * CD\n",
      "\n",
      "try:\n",
      "    print(f\"K_CD={str(K_CD)[:100]}\")\n",
      "except:\n",
      "    pass\n",
      "\n",
      "\n",
      "# Equate the two expressions for K\n",
      "equation = sp.Eq(K, K_CD)\n",
      "\n",
      "try:\n",
      "    print(f\"equation={str(equation)[:100]}\")\n",
      "except:\n",
      "    pass\n",
      "\n",
      "\n",
      "# Solve for CD in terms of a and b\n",
      "CD_expr = sp.solve(equation, CD)[0]\n",
      "\n",
      "try:\n",
      "    print(f\"CD_expr={str(CD_expr)[:100]}\")\n",
      "except:\n",
      "    pass\n",
      "\n",
      "\n",
      "# To maximize CD, we maximize ab, which occurs when a is slightly greater than 65.6 for isosceles triangle\n",
      "a_val = 66\n",
      "\n",
      "try:\n",
      "    print(f\"a_val={str(a_val)[:100]}\")\n",
      "except:\n",
      "    pass\n",
      "\n",
      "b_val = 120\n",
      "\n",
      "try:\n",
      "    print(f\"b_val={str(b_val)[:100]}\")\n",
      "except:\n",
      "    pass\n",
      "\n",
      "\n",
      "# Calculate the maximum CD\n",
      "CD_max = CD_expr.subs({a: a_val, b: b_val})\n",
      "\n",
      "try:\n",
      "    print(f\"CD_max={str(CD_max)[:100]}\")\n",
      "except:\n",
      "    pass\n",
      "\n",
      "\n",
      "print(CD_max)\n",
      "\n",
      "a=a\n",
      "b=b\n",
      "c=120\n",
      "R=100\n",
      "K=3*a*b/10\n",
      "CD=CD\n",
      "K_CD=60.0*CD\n",
      "equation=Eq(3*a*b/10, 60.0*CD)\n",
      "CD_expr=0.005*a*b\n",
      "a_val=66\n",
      "b_val=120\n",
      "CD_max=39.6000000000000\n",
      "39.6000000000000\n",
      "\n",
      "\n",
      "\n",
      "coimport math\n",
      "import numpy as np\n",
      "import sympy as sp\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "coimport math\n",
      "import numpy as np\n",
      "import sympy as sp\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "coimport math\n",
      "import numpy as np\n",
      "import sympy as sp\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "coimport math\n",
      "import numpy as np\n",
      "import sympy as sp\n",
      "from sympy import symbols, solve, sqrt\r\n",
      "\r\n",
      "# Define the variables\r\n",
      "b = symbols('b')\r\n",
      "\n",
      "try:\n",
      "    print(f\"b={str(b)[:100]}\")\n",
      "except:\n",
      "    pass\n",
      "\n",
      "\r\n",
      "# Define the equation for b in terms of known values\r\n",
      "a = 60 * b / 59\n",
      "\n",
      "try:\n",
      "    print(f\"a={str(a)[:100]}\")\n",
      "except:\n",
      "    pass\n",
      "\n",
      "c = 2 * 10 ** 6 + 1\n",
      "\n",
      "try:\n",
      "    print(f\"c={str(c)[:100]}\")\n",
      "except:\n",
      "    pass\n",
      "\n",
      "\n",
      "# Since b^5 = C, we take the fifth root of C to find b\r\n",
      "b_value = c / 4\r\n",
      "\n",
      "try:\n",
      "    print(f\"b_value={str(b_value)[:100]}\")\n",
      "except:\n",
      "    pass\n",
      "\n",
      "\r\n",
      "# calculate b\n",
      "b_value = 3.\n",
      "\n",
      "try:\n",
      "    print(f\"b_value={str(b_value)[:100]}\")\n",
      "except:\n",
      "    pass\n",
      "\n",
      "print(b_value)\n",
      "\n",
      "b=b\n",
      "a=60*b/59\n",
      "c=2000001\n",
      "b_value=500000.25\n",
      "b_value=3.0\n",
      "3.0\n",
      "\n",
      "\n",
      "\n",
      "coimport math\n",
      "import numpy as np\n",
      "import sympy as sp\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 8/8 [02:52<00:00, 21.51s/it, est. speed input: 103.06 toks/s, output: 66.64 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6', '120', '39.6', '23', '36', '249', '96', '1']\n",
      "36\n",
      "\n",
      "\n",
      "\n",
      "Triangle $ABC$ has side length $AB = 120$ and circumradius $R = 100$. Let $D$ be the foot of the perpendicular from $C$ to the line $AB$. What is the greatest possible length of segment $CD$?\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "a1d40b\n",
      "The Fibonacci numbers are defined as follows: $F_0 = 0$, $F_1 = 1$, and $F_{n+1} = F_n + F_{n-1}$ for $n \\geq 1$. There are $N$ positive integers $n$ strictly less than $10^{101}$ such that $n^2 + (n+1)^2$ is a multiple of 5 but $F_{n-1}^2 + F_n^2$ is not. How many prime factors does $N$ have, counted with multiplicity?\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "1fce4b\n",
      "Find the three-digit number $n$ such that writing any other three-digit number $10^{2024}$ times in a row and $10^{2024}+2$ times in a row results in two numbers divisible by $n$.\n",
      "------\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "192e23\n",
      "Fred and George take part in a tennis tournament with $4046$ other players. In each round, the players are paired into $2024$ matches. How many ways are there to arrange the first round such that Fred and George do not have to play each other? (Two arrangements for the first round are \\textit{different} if there is a player with a different opponent in the two arrangements.)\n",
      "------\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inference_server = kaggle_evaluation.aimo_2_inference_server.AIMO2InferenceServer(\n",
    "    predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway((\n",
    "        #             '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/test.csv',\n",
    "        'reference.csv', ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "581fd63a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:16:20.461004Z",
     "iopub.status.busy": "2024-11-16T00:16:20.460713Z",
     "iopub.status.idle": "2024-11-16T00:16:20.466627Z",
     "shell.execute_reply": "2024-11-16T00:16:20.465964Z"
    },
    "papermill": {
     "duration": 0.020674,
     "end_time": "2024-11-16T00:16:20.468253",
     "exception": false,
     "start_time": "2024-11-16T00:16:20.447579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport gc  # garbage collector\\n\\n# clean memory (RAM and GPU memory) to avoid memory leaks and out-of-memory errors (e.g., due to PyTorch tensor parallelism)\\n# and improve performance (e.g., by reducing memory fragmentation) and stability (e.g., by avoiding memory leaks) of the VLLM model\\ndef clean_memory(deep=False):\\n    gc.collect()  # garbage collector (RAM)\\n    if deep:\\n        # memory allocator (RAM) for PyTorch tensors\\n        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\\n    # memory allocator (GPU) for PyTorch tensors\\n    torch.cuda.empty_cache()\\n\\n\\n# delete the VLLM model to free up GPU memory\\ndel llm\\n\\n# clean memory (RAM and GPU memory) to avoid memory leaks and out-of-memory errors\\nclean_memory(deep=True)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import gc  # garbage collector\n",
    "\n",
    "# clean memory (RAM and GPU memory) to avoid memory leaks and out-of-memory errors (e.g., due to PyTorch tensor parallelism)\n",
    "# and improve performance (e.g., by reducing memory fragmentation) and stability (e.g., by avoiding memory leaks) of the VLLM model\n",
    "def clean_memory(deep=False):\n",
    "    gc.collect()  # garbage collector (RAM)\n",
    "    if deep:\n",
    "        # memory allocator (RAM) for PyTorch tensors\n",
    "        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    # memory allocator (GPU) for PyTorch tensors\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# delete the VLLM model to free up GPU memory\n",
    "del llm\n",
    "\n",
    "# clean memory (RAM and GPU memory) to avoid memory leaks and out-of-memory errors\n",
    "clean_memory(deep=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dcf064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified file handling code\n",
    "def save_and_display_files():\n",
    "    \"\"\"Save files and create reliable download links\"\"\"\n",
    "    import os\n",
    "    from IPython.display import HTML, FileLink, display\n",
    "    import shutil\n",
    "\n",
    "    # Ensure we're in the Kaggle working directory\n",
    "    working_dir = \"/kaggle/working\"\n",
    "    if not os.path.exists(working_dir):\n",
    "        print(\"Error: Not running in Kaggle environment\")\n",
    "        return\n",
    "\n",
    "    # Function to safely handle file copy\n",
    "    def safe_copy(src, dst):\n",
    "        try:\n",
    "            shutil.copy2(src, dst)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error copying {src}: {e}\")\n",
    "            return False\n",
    "\n",
    "    # Create a new directory for our outputs\n",
    "    output_dir = os.path.join(working_dir, \"outputs\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # List and copy all relevant files\n",
    "    files_copied = []\n",
    "    for root, _, files in os.walk(working_dir):\n",
    "        for filename in files:\n",
    "            # Skip the outputs directory itself and any temp files\n",
    "            if \"outputs\" in root or filename.startswith('.'):\n",
    "                continue\n",
    "\n",
    "            src_path = os.path.join(root, filename)\n",
    "            dst_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            if safe_copy(src_path, dst_path):\n",
    "                files_copied.append(filename)\n",
    "\n",
    "    # Create HTML display for available files\n",
    "    if files_copied:\n",
    "        print(\"\\nFiles available for download:\")\n",
    "        for filename in sorted(files_copied):\n",
    "            try:\n",
    "                # Create FileLink for each file\n",
    "                file_path = os.path.join(\"outputs\", filename)\n",
    "                display(FileLink(file_path,\n",
    "                                 result_html_prefix=f\"{filename}: \"))\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating link for {filename}: {e}\")\n",
    "    else:\n",
    "        print(\"No files were found to copy\")\n",
    "\n",
    "    # Create zip file of all outputs\n",
    "    try:\n",
    "        zip_path = os.path.join(working_dir, \"outputs.zip\")\n",
    "        shutil.make_archive(os.path.join(working_dir, \"outputs\"), 'zip',\n",
    "                            output_dir)\n",
    "        print(\"\\nAll files are also available in a single zip:\")\n",
    "        display(\n",
    "            FileLink(\"outputs.zip\", result_html_prefix=\"Download all files: \"))\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating zip file: {e}\")\n",
    "\n",
    "\n",
    "# Run the function\n",
    "save_and_display_files()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 9869096,
     "sourceId": 86023,
     "sourceType": "competition"
    },
    {
     "sourceId": 205183965,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 127417,
     "modelInstanceId": 118183,
     "sourceId": 139552,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 153931,
     "modelInstanceId": 131113,
     "sourceId": 154372,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 784.570244,
   "end_time": "2024-11-16T00:16:24.201766",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-16T00:03:19.631522",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "005bee3e5e224797aae923cc069af943": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_83f19028923a42f491d0c4ec8a931089",
        "IPY_MODEL_43be2a1ba93643089061270ea8e066a5",
        "IPY_MODEL_64679f64d68042fe8b4feb4e69fb88b5"
       ],
       "layout": "IPY_MODEL_74db17bad11347d5836ee5ba6317c615"
      }
     },
     "43be2a1ba93643089061270ea8e066a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c8db1f4d0876494e8767b5a30a44abdb",
       "max": 9,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e82e06cf83834182865fd3ea6849836a",
       "value": 9
      }
     },
     "64679f64d68042fe8b4feb4e69fb88b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bff736f67b344688bca230ba0ab323e6",
       "placeholder": "​",
       "style": "IPY_MODEL_9388b4420c734248a04af29ae2919afd",
       "value": "Loading safetensors checkpoint shards: 100% Completed | 9/9 [05:08&lt;00:00, 36.72s/it]\n"
      }
     },
     "6a7b4daad0664d65adcfb467d6102dbf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "74db17bad11347d5836ee5ba6317c615": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "83f19028923a42f491d0c4ec8a931089": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c645a5985f5140ad942f6a3f3f49bf64",
       "placeholder": "​",
       "style": "IPY_MODEL_6a7b4daad0664d65adcfb467d6102dbf",
       "value": ""
      }
     },
     "9388b4420c734248a04af29ae2919afd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "bff736f67b344688bca230ba0ab323e6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c645a5985f5140ad942f6a3f3f49bf64": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c8db1f4d0876494e8767b5a30a44abdb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e82e06cf83834182865fd3ea6849836a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
