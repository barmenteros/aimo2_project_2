{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab52f42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:03:23.912025Z",
     "iopub.status.busy": "2024-11-16T00:03:23.911338Z",
     "iopub.status.idle": "2024-11-16T00:03:41.934219Z",
     "shell.execute_reply": "2024-11-16T00:03:41.933429Z"
    },
    "papermill": {
     "duration": 18.031768,
     "end_time": "2024-11-16T00:03:41.936182",
     "exception": false,
     "start_time": "2024-11-16T00:03:23.904414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import torch\n",
    "import kaggle_evaluation.aimo_2_inference_server\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "cutoff_time = time.time() + (4 * 60 + 55) * 60  # 4 hours 55 minutes from now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fca8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def log_model_interaction(messages,\n",
    "                          response,\n",
    "                          log_file=\"model_interactions.jsonl\"):\n",
    "    log_entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"input_messages\": messages,\n",
    "        \"model_response\": response\n",
    "    }\n",
    "    with open(log_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(log_entry, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9347702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(seconds: float) -> str:\n",
    "    \"\"\"Convert seconds to HH:MM:SS.mm format\"\"\"\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    seconds_remainder = seconds % 60\n",
    "    \n",
    "    if hours > 0:\n",
    "        return f\"{hours:02d}:{minutes:02d}:{seconds_remainder:05.2f}\"\n",
    "    elif minutes > 0:\n",
    "        return f\"{minutes:02d}:{seconds_remainder:05.2f}\"\n",
    "    return f\"{seconds_remainder:.2f} seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d413ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "\n",
    "def problem_timer(func):\n",
    "\n",
    "    @wraps(func)\n",
    "    def wrapper(question: str, id_: str, *args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(question, id_, *args, **kwargs)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Problem {id_} completed in: {format_time(elapsed_time)}\")\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160cb9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeTimer:\n",
    "    def __init__(self, description=\"Code block\"):\n",
    "        self.description = description\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter()\n",
    "        return self\n",
    "               \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.end = time.perf_counter()\n",
    "        self.duration = self.end - self.start\n",
    "        print(f\"\\n{self.description} completed in: {format_time(self.duration)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3858e4f",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-11-16T00:03:41.950030Z",
     "iopub.status.busy": "2024-11-16T00:03:41.949621Z",
     "iopub.status.idle": "2024-11-16T00:04:27.190263Z",
     "shell.execute_reply": "2024-11-16T00:04:27.189470Z"
    },
    "papermill": {
     "duration": 45.249755,
     "end_time": "2024-11-16T00:04:27.192425",
     "exception": false,
     "start_time": "2024-11-16T00:03:41.942670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with CodeTimer(\"VLLM Model & CUDA Setup\"):\n",
    "    from vllm import LLM, SamplingParams \n",
    "\n",
    "    warnings.simplefilter('ignore')\n",
    "\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f19b67a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:04:27.206650Z",
     "iopub.status.busy": "2024-11-16T00:04:27.206046Z",
     "iopub.status.idle": "2024-11-16T00:10:26.631894Z",
     "shell.execute_reply": "2024-11-16T00:10:26.631031Z"
    },
    "papermill": {
     "duration": 359.435269,
     "end_time": "2024-11-16T00:10:26.634241",
     "exception": false,
     "start_time": "2024-11-16T00:04:27.198972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with CodeTimer(\"Model initialization\"):\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "\n",
    "    llm_model_pth = '/kaggle/input/qwen_qwen2.5_math_72bawq2/transformers/72b/3'\n",
    "\n",
    "    llm = LLM(\n",
    "        llm_model_pth,\n",
    "        dtype=torch.bfloat16,\n",
    "        max_num_seqs=16,\n",
    "        max_model_len=4096, #model max 16384\n",
    "        trust_remote_code=True,\n",
    "        tensor_parallel_size=4,\n",
    "        swap_space=max(1, num_gpus // 2),\n",
    "        gpu_memory_utilization=0.97,\n",
    "        seed=2024,\n",
    "        enforce_eager=True,        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97d0f00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:10:26.656933Z",
     "iopub.status.busy": "2024-11-16T00:10:26.656599Z",
     "iopub.status.idle": "2024-11-16T00:10:26.660336Z",
     "shell.execute_reply": "2024-11-16T00:10:26.659676Z"
    },
    "papermill": {
     "duration": 0.01685,
     "end_time": "2024-11-16T00:10:26.661946",
     "exception": false,
     "start_time": "2024-11-16T00:10:26.645096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9def298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_python_code(text):\n",
    "    pattern = r'```python\\s*(.*?)\\s*```'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return \"\\n\\n\".join(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf23c390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keyword\n",
    "\n",
    "\n",
    "def process_python_code(query):\n",
    "    query = \"import math\\nimport numpy as np\\nimport sympy as sp\\n\" + query\n",
    "    current_rows = query.strip().split(\"\\n\")\n",
    "    new_rows = []\n",
    "    for row in current_rows:\n",
    "        new_rows.append(row)\n",
    "        if not row.startswith(\" \") and \"=\" in row:\n",
    "            variables_to_print = row.split(\"=\")[0].strip()\n",
    "            for variable_to_print in variables_to_print.split(\",\"):\n",
    "                variable_to_print = variable_to_print.strip()\n",
    "                if variable_to_print.isidentifier(\n",
    "                ) and not keyword.iskeyword(variable_to_print):\n",
    "                    if row.count(\"(\") == row.count(\")\") and row.count(\n",
    "                            \"[\") == row.count(\"]\"):\n",
    "                        # TODO: use some AST to parse code\n",
    "                        new_rows.append(\n",
    "                            f'\\ntry:\\n    print(f\"{variable_to_print}={{str({variable_to_print})[:100]}}\")\\nexcept:\\n    pass\\n'\n",
    "                        )\n",
    "    return \"\\n\".join(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1db9f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_boxed_text(text):\n",
    "    pattern = r'oxed{(.*?)}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    return matches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35894c7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:10:26.684072Z",
     "iopub.status.busy": "2024-11-16T00:10:26.683814Z",
     "iopub.status.idle": "2024-11-16T00:10:26.694095Z",
     "shell.execute_reply": "2024-11-16T00:10:26.693445Z"
    },
    "papermill": {
     "duration": 0.023215,
     "end_time": "2024-11-16T00:10:26.695640",
     "exception": false,
     "start_time": "2024-11-16T00:10:26.672425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "\n",
    "def select_answer(answers):\n",
    "    counter = Counter()\n",
    "    for answer in answers:\n",
    "        try:\n",
    "            if int(answer) == float(answer):\n",
    "                counter[int(answer)] += 1 + random.random() / 1_000\n",
    "        except:\n",
    "            pass\n",
    "    if not counter:\n",
    "        return 210\n",
    "    _, answer = sorted([(v, k) for k, v in counter.items()], reverse=True)[0]\n",
    "    return answer % 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6718b62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:10:26.717808Z",
     "iopub.status.busy": "2024-11-16T00:10:26.717188Z",
     "iopub.status.idle": "2024-11-16T00:10:26.725130Z",
     "shell.execute_reply": "2024-11-16T00:10:26.724492Z"
    },
    "papermill": {
     "duration": 0.020748,
     "end_time": "2024-11-16T00:10:26.726770",
     "exception": false,
     "start_time": "2024-11-16T00:10:26.706022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import subprocess\n",
    "\n",
    "\n",
    "class PythonREPL:\n",
    "\n",
    "    def __init__(self, timeout=5):\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def __call__(self, query):\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            temp_file_path = os.path.join(temp_dir, \"tmp.py\")\n",
    "            with open(temp_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(query)\n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [\"python3\", temp_file_path],\n",
    "                    capture_output=True,\n",
    "                    check=False,\n",
    "                    text=True,\n",
    "                    timeout=self.timeout,\n",
    "                )\n",
    "            except subprocess.TimeoutExpired:\n",
    "                return False, f\"Execution timed out after {self.timeout} seconds.\"\n",
    "            stdout = result.stdout.strip()\n",
    "            stderr = result.stderr.strip()\n",
    "            if result.returncode == 0:\n",
    "                return True, stdout\n",
    "            else:\n",
    "                error_lines = stderr.split(\"\\n\")\n",
    "                cleaned_errors = []\n",
    "                for line in error_lines:\n",
    "                    if temp_file_path in line:\n",
    "                        line = line.replace(temp_file_path, \"<temporary_file>\")\n",
    "                    cleaned_errors.append(line)\n",
    "                cleaned_error_msg = \"\\n\".join(cleaned_errors)\n",
    "                combined_output = f\"{stdout}\\n{cleaned_error_msg}\" if stdout else cleaned_error_msg\n",
    "                return False, combined_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1d2380",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:10:26.748868Z",
     "iopub.status.busy": "2024-11-16T00:10:26.748299Z",
     "iopub.status.idle": "2024-11-16T00:10:26.754432Z",
     "shell.execute_reply": "2024-11-16T00:10:26.753794Z"
    },
    "papermill": {
     "duration": 0.018873,
     "end_time": "2024-11-16T00:10:26.756027",
     "exception": false,
     "start_time": "2024-11-16T00:10:26.737154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    # min_p=0.01,\n",
    "    top_p=0.8,\n",
    "    #skip_special_tokens=True,\n",
    "    max_tokens=3072,\n",
    "    stop=[\"```\\n\"],\n",
    "    include_stop_str_in_output=True,\n",
    ")\n",
    "\n",
    "\n",
    "def batch_message_generate(list_of_messages) -> list[list[dict]]:\n",
    "    list_of_texts = [\n",
    "        tokenizer.apply_chat_template(conversation=messages,\n",
    "                                      tokenize=False,\n",
    "                                      add_generation_prompt=True)\n",
    "        for messages in list_of_messages\n",
    "    ]\n",
    "\n",
    "    request_output = llm.generate(\n",
    "        prompts=list_of_texts,\n",
    "        sampling_params=sampling_params,\n",
    "        use_tqdm=True,\n",
    "    )\n",
    "\n",
    "    for messages, full_prompt, single_request_output in zip(\n",
    "            list_of_messages, list_of_texts, request_output):\n",
    "        print(\"\\n=== Input Prompt ===\")\n",
    "        print(full_prompt)\n",
    "        print(\"==================\\n\")\n",
    "\n",
    "        response_text = single_request_output.outputs[0].text\n",
    "        print(\"=== Model Response ===\")\n",
    "        print(response_text)\n",
    "        print(\"=====================\\n\")\n",
    "\n",
    "        log_model_interaction(messages, response_text)\n",
    "        messages.append({'role': 'assistant', 'content': response_text})\n",
    "\n",
    "    return list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0d7342",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:10:26.778196Z",
     "iopub.status.busy": "2024-11-16T00:10:26.777625Z",
     "iopub.status.idle": "2024-11-16T00:10:26.782030Z",
     "shell.execute_reply": "2024-11-16T00:10:26.781387Z"
    },
    "papermill": {
     "duration": 0.017004,
     "end_time": "2024-11-16T00:10:26.783582",
     "exception": false,
     "start_time": "2024-11-16T00:10:26.766578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_message_filter(\n",
    "        list_of_messages) -> tuple[list[list[dict]], list[str]]:\n",
    "    extracted_answers = []\n",
    "    list_of_messages_to_keep = []\n",
    "    for messages in list_of_messages:\n",
    "        answer = extract_boxed_text(messages[-1]['content'])\n",
    "        if answer:\n",
    "            extracted_answers.append(answer)\n",
    "        else:\n",
    "            list_of_messages_to_keep.append(messages)\n",
    "    return list_of_messages_to_keep, extracted_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027092c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:10:26.805645Z",
     "iopub.status.busy": "2024-11-16T00:10:26.805076Z",
     "iopub.status.idle": "2024-11-16T00:10:26.811371Z",
     "shell.execute_reply": "2024-11-16T00:10:26.810757Z"
    },
    "papermill": {
     "duration": 0.019087,
     "end_time": "2024-11-16T00:10:26.812953",
     "exception": false,
     "start_time": "2024-11-16T00:10:26.793866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_message_execute(list_of_messages) -> list[list[dict]]:\n",
    "    for messages in list_of_messages:\n",
    "        python_code = extract_python_code(messages[-1]['content'])\n",
    "        python_code = process_python_code(python_code)\n",
    "        # print('\\n\\n' + python_code + '\\n\\n')\n",
    "        try:\n",
    "            print('c', end='')\n",
    "            is_successful, output = PythonREPL()(python_code)\n",
    "            if is_successful:\n",
    "                print('o', end='')\n",
    "            else:\n",
    "                print('e', end='')\n",
    "        except Exception as e:\n",
    "            print('f', end='')\n",
    "            output = str(e)\n",
    "        print(python_code)\n",
    "        print()\n",
    "        print(output)\n",
    "        print(\"\\n\\n\")\n",
    "        messages.append({\n",
    "            'role': 'user',\n",
    "            'content': \"```output\\n\" + output + \"\\n```\"\n",
    "        })\n",
    "    print()\n",
    "    return list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e265ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:10:26.834746Z",
     "iopub.status.busy": "2024-11-16T00:10:26.834197Z",
     "iopub.status.idle": "2024-11-16T00:10:26.838858Z",
     "shell.execute_reply": "2024-11-16T00:10:26.838211Z"
    },
    "papermill": {
     "duration": 0.017196,
     "end_time": "2024-11-16T00:10:26.840454",
     "exception": false,
     "start_time": "2024-11-16T00:10:26.823258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class MessageStyle(Enum):\n",
    "    COT = \"chain_of_thought\"\n",
    "    TIR = \"tool_integrated_reasoning\"\n",
    "    ALTERNATE = \"alternate\"\n",
    "\n",
    "\n",
    "def create_starter_messages(question, index, style=MessageStyle.ALTERNATE):\n",
    "    # https://github.com/QwenLM/Qwen2.5-Math?tab=readme-ov-file#-hugging-face-transformers\n",
    "    cot_message = [{\n",
    "        \"role\":\n",
    "        \"system\",\n",
    "        \"content\":\n",
    "        \"Please reason step by step, and put your final answer within \\\\boxed{}.\"\n",
    "    }, {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": question\n",
    "    }]\n",
    "\n",
    "    tir_message = [{\n",
    "        \"role\":\n",
    "        \"system\",\n",
    "        \"content\":\n",
    "        \"Please integrate natural language reasoning with programs to solve the problem above, and put your final answer within \\\\boxed{}.\"\n",
    "    }, {\n",
    "        \"role\":\n",
    "        \"user\",\n",
    "        \"content\":\n",
    "        question + \"\\n\\nBegin your answer by importing math and sympy.\"\n",
    "    }]\n",
    "\n",
    "    if style == MessageStyle.COT:\n",
    "        return cot_message\n",
    "    elif style == MessageStyle.TIR:\n",
    "        return tir_message\n",
    "    else:  # ALTERNATE\n",
    "        cycle_size = 2  # Preserved for alternate mode\n",
    "        return cot_message if index % cycle_size == 1 else tir_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8da89aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:10:26.892177Z",
     "iopub.status.busy": "2024-11-16T00:10:26.891631Z",
     "iopub.status.idle": "2024-11-16T00:10:26.897976Z",
     "shell.execute_reply": "2024-11-16T00:10:26.897325Z"
    },
    "papermill": {
     "duration": 0.018983,
     "end_time": "2024-11-16T00:10:26.899551",
     "exception": false,
     "start_time": "2024-11-16T00:10:26.880568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@problem_timer\n",
    "def predict_for_question(question: str, id_: str) -> int:\n",
    "    if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        if id_ != \"1acac0\" and id_ != \"480182\":\n",
    "            return 210\n",
    "    if time.time() > cutoff_time:\n",
    "        return 210\n",
    "    question += \"\\nImportant: If your calculations result in a number greater than 1e6, \" \\\n",
    "                \"this likely indicates an error - please review your approach. \" \\\n",
    "                \"For calculations of $x \\\\bmod m$, ensure your result $r$ satisfies $0 \\\\leq r < m$.\" \n",
    "    num_messages = 6\n",
    "    list_of_messages = [\n",
    "        # Only CoT messages\n",
    "        create_starter_messages(question, index, MessageStyle.COT)\n",
    "        for index in range(num_messages)\n",
    "    ]\n",
    "    all_extracted_answers = []\n",
    "    # 4 rounds of message generation, filtering, and execution\n",
    "    for _ in range(4):\n",
    "        list_of_messages = batch_message_generate(list_of_messages)\n",
    "        list_of_messages, extracted_answers = batch_message_filter(\n",
    "            list_of_messages)\n",
    "        all_extracted_answers.extend(extracted_answers)\n",
    "        if not list_of_messages:\n",
    "            break\n",
    "        list_of_messages = batch_message_execute(list_of_messages)\n",
    "    print(all_extracted_answers)\n",
    "    answer = select_answer(all_extracted_answers)\n",
    "    print(answer)\n",
    "    print(\"\\n\\n\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec2bf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:10:26.921092Z",
     "iopub.status.busy": "2024-11-16T00:10:26.920841Z",
     "iopub.status.idle": "2024-11-16T00:10:26.925292Z",
     "shell.execute_reply": "2024-11-16T00:10:26.924666Z"
    },
    "papermill": {
     "duration": 0.017005,
     "end_time": "2024-11-16T00:10:26.926884",
     "exception": false,
     "start_time": "2024-11-16T00:10:26.909879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace this function with your inference code.\n",
    "# The function should return a single integer between 0 and 999, inclusive.\n",
    "# Each prediction (except the very first) must be returned within 30 minutes of the question being provided.\n",
    "def predict(id_: pl.DataFrame,\n",
    "            question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
    "    id_ = id_.item(0)\n",
    "    print(\"------\")\n",
    "    print(id_)\n",
    "    question = question.item(0)\n",
    "    answer = predict_for_question(question, id_)\n",
    "    print(question)\n",
    "    print(\"------\\n\\n\\n\")\n",
    "    return pl.DataFrame({'id': id_, 'answer': answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34de22c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:10:26.949528Z",
     "iopub.status.busy": "2024-11-16T00:10:26.948925Z",
     "iopub.status.idle": "2024-11-16T00:10:26.952307Z",
     "shell.execute_reply": "2024-11-16T00:10:26.951683Z"
    },
    "papermill": {
     "duration": 0.016078,
     "end_time": "2024-11-16T00:10:26.953890",
     "exception": false,
     "start_time": "2024-11-16T00:10:26.937812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict_for_question(\"Triangle $ABC$ has side length $AB = 120$ and circumradius $R = 100$. Let $D$ be the foot of the perpendicular from $C$ to the line $AB$. What is the greatest possible length of segment $CD$?\", \"1acac0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34c3214",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:10:26.975902Z",
     "iopub.status.busy": "2024-11-16T00:10:26.975422Z",
     "iopub.status.idle": "2024-11-16T00:10:27.032673Z",
     "shell.execute_reply": "2024-11-16T00:10:27.031977Z"
    },
    "papermill": {
     "duration": 0.070097,
     "end_time": "2024-11-16T00:10:27.034433",
     "exception": false,
     "start_time": "2024-11-16T00:10:26.964336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\n",
    "    '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/reference.csv'\n",
    ").drop('answer', axis=1).to_csv('reference.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f6ec46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:10:27.056592Z",
     "iopub.status.busy": "2024-11-16T00:10:27.056332Z",
     "iopub.status.idle": "2024-11-16T00:16:20.433439Z",
     "shell.execute_reply": "2024-11-16T00:16:20.432642Z"
    },
    "papermill": {
     "duration": 353.39044,
     "end_time": "2024-11-16T00:16:20.435392",
     "exception": false,
     "start_time": "2024-11-16T00:10:27.044952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_server = kaggle_evaluation.aimo_2_inference_server.AIMO2InferenceServer(\n",
    "    predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(('reference.csv', ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581fd63a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T00:16:20.461004Z",
     "iopub.status.busy": "2024-11-16T00:16:20.460713Z",
     "iopub.status.idle": "2024-11-16T00:16:20.466627Z",
     "shell.execute_reply": "2024-11-16T00:16:20.465964Z"
    },
    "papermill": {
     "duration": 0.020674,
     "end_time": "2024-11-16T00:16:20.468253",
     "exception": false,
     "start_time": "2024-11-16T00:16:20.447579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import gc  # garbage collector\n",
    "\n",
    "# clean memory (RAM and GPU memory) to avoid memory leaks and out-of-memory errors (e.g., due to PyTorch tensor parallelism)\n",
    "# and improve performance (e.g., by reducing memory fragmentation) and stability (e.g., by avoiding memory leaks) of the VLLM model\n",
    "def clean_memory(deep=False):\n",
    "    gc.collect()  # garbage collector (RAM)\n",
    "    if deep:\n",
    "        # memory allocator (RAM) for PyTorch tensors\n",
    "        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    # memory allocator (GPU) for PyTorch tensors\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# delete the VLLM model to free up GPU memory\n",
    "del llm\n",
    "\n",
    "# clean memory (RAM and GPU memory) to avoid memory leaks and out-of-memory errors\n",
    "clean_memory(deep=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dcf064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified file handling code\n",
    "def save_and_display_files():\n",
    "    \"\"\"Save files and create reliable download links\"\"\"\n",
    "    import os\n",
    "    from IPython.display import HTML, FileLink, display\n",
    "    import shutil\n",
    "\n",
    "    # Ensure we're in the Kaggle working directory\n",
    "    working_dir = \"/kaggle/working\"\n",
    "    if not os.path.exists(working_dir):\n",
    "        print(\"Error: Not running in Kaggle environment\")\n",
    "        return\n",
    "\n",
    "    # Function to safely handle file copy\n",
    "    def safe_copy(src, dst):\n",
    "        try:\n",
    "            shutil.copy2(src, dst)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error copying {src}: {e}\")\n",
    "            return False\n",
    "\n",
    "    # Create a new directory for our outputs\n",
    "    output_dir = os.path.join(working_dir, \"outputs\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # List and copy all relevant files\n",
    "    files_copied = []\n",
    "    for root, _, files in os.walk(working_dir):\n",
    "        for filename in files:\n",
    "            # Skip the outputs directory itself and any temp files\n",
    "            if \"outputs\" in root or filename.startswith('.'):\n",
    "                continue\n",
    "\n",
    "            src_path = os.path.join(root, filename)\n",
    "            dst_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            if safe_copy(src_path, dst_path):\n",
    "                files_copied.append(filename)\n",
    "\n",
    "    # Create HTML display for available files\n",
    "    if files_copied:\n",
    "        print(\"\\nFiles available for download:\")\n",
    "        for filename in sorted(files_copied):\n",
    "            try:\n",
    "                # Create FileLink for each file\n",
    "                file_path = os.path.join(\"outputs\", filename)\n",
    "                display(FileLink(file_path,\n",
    "                                 result_html_prefix=f\"{filename}: \"))\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating link for {filename}: {e}\")\n",
    "    else:\n",
    "        print(\"No files were found to copy\")\n",
    "\n",
    "    # Create zip file of all outputs\n",
    "    try:\n",
    "        zip_path = os.path.join(working_dir, \"outputs.zip\")\n",
    "        shutil.make_archive(os.path.join(working_dir, \"outputs\"), 'zip',\n",
    "                            output_dir)\n",
    "        print(\"\\nAll files are also available in a single zip:\")\n",
    "        display(\n",
    "            FileLink(\"outputs.zip\", result_html_prefix=\"Download all files: \"))\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating zip file: {e}\")\n",
    "\n",
    "\n",
    "# Run the function\n",
    "save_and_display_files()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 9869096,
     "sourceId": 86023,
     "sourceType": "competition"
    },
    {
     "sourceId": 205183965,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 127417,
     "modelInstanceId": 118183,
     "sourceId": 139552,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 153931,
     "modelInstanceId": 131113,
     "sourceId": 154372,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 784.570244,
   "end_time": "2024-11-16T00:16:24.201766",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-16T00:03:19.631522",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "005bee3e5e224797aae923cc069af943": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_83f19028923a42f491d0c4ec8a931089",
        "IPY_MODEL_43be2a1ba93643089061270ea8e066a5",
        "IPY_MODEL_64679f64d68042fe8b4feb4e69fb88b5"
       ],
       "layout": "IPY_MODEL_74db17bad11347d5836ee5ba6317c615"
      }
     },
     "43be2a1ba93643089061270ea8e066a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c8db1f4d0876494e8767b5a30a44abdb",
       "max": 9,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e82e06cf83834182865fd3ea6849836a",
       "value": 9
      }
     },
     "64679f64d68042fe8b4feb4e69fb88b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bff736f67b344688bca230ba0ab323e6",
       "placeholder": "​",
       "style": "IPY_MODEL_9388b4420c734248a04af29ae2919afd",
       "value": "Loading safetensors checkpoint shards: 100% Completed | 9/9 [05:08&lt;00:00, 36.72s/it]\n"
      }
     },
     "6a7b4daad0664d65adcfb467d6102dbf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "74db17bad11347d5836ee5ba6317c615": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "83f19028923a42f491d0c4ec8a931089": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c645a5985f5140ad942f6a3f3f49bf64",
       "placeholder": "​",
       "style": "IPY_MODEL_6a7b4daad0664d65adcfb467d6102dbf",
       "value": ""
      }
     },
     "9388b4420c734248a04af29ae2919afd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "bff736f67b344688bca230ba0ab323e6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c645a5985f5140ad942f6a3f3f49bf64": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c8db1f4d0876494e8767b5a30a44abdb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e82e06cf83834182865fd3ea6849836a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
